{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "453dd7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       AAPL      AMZN      GOOG      MSFT\n",
      "0 -0.000076  0.005513  0.007070  0.002016\n",
      "1  0.002013  0.001940  0.001570  0.003806\n",
      "2  0.004917  0.006963  0.006282  0.005351\n",
      "3 -0.001616  0.006220  0.001852  0.000443\n",
      "4 -0.000050  0.002026 -0.000267 -0.000295\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# load data from log-returns\n",
    "\n",
    "df_log_returns = pd.read_csv('squashed_log_returns.csv')\n",
    "\n",
    "print(df_log_returns.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f46b86d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   context                                           features  raw_label\n",
      "0        0  [-7.570098556833607e-05, 0.002012513590532, 0....  -0.001616\n",
      "1        0  [0.002012513590532, 0.0049166646207804, -0.001...  -0.000050\n",
      "2        0  [0.0049166646207804, -0.0016160763885611, -4.9...  -0.000100\n",
      "3        0  [-0.0016160763885611, -4.977979782626501e-05, ...   0.002460\n",
      "4        0  [-4.977979782626501e-05, -9.973891197457896e-0...   0.004462\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#split into windows\n",
    "WINDOW_SIZE = 4 #time windows of 4\n",
    "\n",
    "df = pd.DataFrame(columns=['context','features','raw_label'])\n",
    "    \n",
    "for context, ticker in enumerate(df_log_returns.columns):\n",
    "        \n",
    "    stock_series = df_log_returns[ticker]\n",
    "        \n",
    "    # Slide a window across this one stock's time series\n",
    "    # We stop (window_size - 1) from the end\n",
    "    for i in range(len(stock_series) - WINDOW_SIZE + 1):\n",
    "            \n",
    "        # The full window (e.g., 4 log-returns)\n",
    "        window = stock_series.iloc[i : i + WINDOW_SIZE]\n",
    "            \n",
    "        # Features are the first N-1 (e.g., 3)\n",
    "        features = window[:-1].values\n",
    "            \n",
    "        # Label is the last one (e.g., the 4th)\n",
    "        label = window.iloc[-1]\n",
    "\n",
    "        df.loc[len(df)] = [context, features, label]\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a93f434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    context                                           features  raw_label  \\\n",
      "0         0  [-7.570098556833607e-05, 0.002012513590532, 0....  -0.001616   \n",
      "1         0  [0.002012513590532, 0.0049166646207804, -0.001...  -0.000050   \n",
      "2         0  [0.0049166646207804, -0.0016160763885611, -4.9...  -0.000100   \n",
      "3         0  [-0.0016160763885611, -4.977979782626501e-05, ...   0.002460   \n",
      "4         0  [-4.977979782626501e-05, -9.973891197457896e-0...   0.004462   \n",
      "5         0  [-9.973891197457896e-05, 0.0024599308278872, 0...  -0.002213   \n",
      "6         0  [0.0024599308278872, 0.0044615553560915, -0.00...   0.007114   \n",
      "7         0  [0.0044615553560915, -0.0022127250514393, 0.00...   0.000388   \n",
      "8         0  [-0.0022127250514393, 0.007114255558871, 0.000...  -0.001942   \n",
      "9         0  [0.007114255558871, 0.0003877178948868, -0.001...  -0.003568   \n",
      "10        0  [0.0003877178948868, -0.001942374603717, -0.00...   0.000098   \n",
      "11        0  [-0.001942374603717, -0.0035676390655656, 9.81...  -0.006973   \n",
      "12        0  [-0.0035676390655656, 9.81321574251727e-05, -0...  -0.007823   \n",
      "13        0  [9.81321574251727e-05, -0.0069732283076202, -0...   0.001014   \n",
      "14        0  [-0.0069732283076202, -0.0078225331054288, 0.0...  -0.009084   \n",
      "15        0  [-0.0078225331054288, 0.0010141954091522, -0.0...  -0.002567   \n",
      "16        0  [0.0010141954091522, -0.0090835460611317, -0.0...   0.001195   \n",
      "17        0  [-0.0090835460611317, -0.0025672026456246, 0.0...   0.000907   \n",
      "18        0  [-0.0025672026456246, 0.0011947578118953, 0.00...  -0.019263   \n",
      "19        0  [0.0011947578118953, 0.0009069114032109, -0.01...  -0.010988   \n",
      "\n",
      "    label  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n",
      "5       1  \n",
      "6       1  \n",
      "7       1  \n",
      "8       1  \n",
      "9       1  \n",
      "10      1  \n",
      "11      0  \n",
      "12      0  \n",
      "13      1  \n",
      "14      0  \n",
      "15      1  \n",
      "16      1  \n",
      "17      1  \n",
      "18      0  \n",
      "19      0  \n"
     ]
    }
   ],
   "source": [
    "# Convert labels to bins\n",
    "N_BINS = 2\n",
    "\n",
    "context_min = df.groupby('context')['raw_label'].transform('min')\n",
    "context_max = df.groupby('context')['raw_label'].transform('max')\n",
    "\n",
    "midpoint = (context_min + context_max) / 2\n",
    "\n",
    "df['label'] = (df['raw_label'] > midpoint).astype(int)\n",
    "\n",
    "print(df.head(20))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d15acdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4816, 4)\n",
      "(1204, 4)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "\n",
    "TRAIN_RATIO = 0.8  # take the first 80% of df as requested\n",
    "\n",
    "# Compute split index using the row order (first 80%)\n",
    "n_rows = len(df)\n",
    "split_index = int(n_rows * TRAIN_RATIO)\n",
    "\n",
    "# First 80% (preserve original order); create df_test for remainder\n",
    "df_train = df.iloc[:split_index].reset_index(drop=True)\n",
    "df_test = df.iloc[split_index:].reset_index(drop=True)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4eb426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "#Build the circuit\n",
    "\n",
    "n_stocks = df_log_returns.shape[1]\n",
    "\n",
    "# 0-1: context qubits\n",
    "# 2-4: input qubits\n",
    "# 5: output qubit\n",
    "N_CONTEXT_WIRES = int(np.log2(n_stocks))  # Number of qubits needed to represent stocks\n",
    "N_INPUT_WIRES = df_train['features'].loc[0].shape[0]  # Should be 3\n",
    "N_OUTPUT_WIRES = int(np.log2(N_BINS))\n",
    "N_TOTAL_WIRES = N_CONTEXT_WIRES + N_INPUT_WIRES + N_OUTPUT_WIRES\n",
    "\n",
    "N_LAYERS = 2\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "N_EPOCHS = 10\n",
    "\n",
    "CONTEXT_WIRES = list(range(N_CONTEXT_WIRES))\n",
    "INPUT_WIRES = list(range(N_CONTEXT_WIRES, N_CONTEXT_WIRES + N_INPUT_WIRES))\n",
    "OUTPUT_WIRES = list(range(N_CONTEXT_WIRES + N_INPUT_WIRES, N_TOTAL_WIRES))\n",
    "COMP_WIRES = list(range(N_CONTEXT_WIRES, N_CONTEXT_WIRES + N_OUTPUT_WIRES + N_INPUT_WIRES))\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=6)\n",
    "\n",
    "#input layer\n",
    "def U_in(features):\n",
    "    \"\"\"The data encoding block (feature map).\"\"\"\n",
    "    # We use arctan to squash features, as discussed\n",
    "    for i, wire in enumerate(INPUT_WIRES):\n",
    "        qml.RY(features[i], wires=wire)\n",
    "\n",
    "# layer for both either the shared or specify ansatz (input is just trainable parameters)\n",
    "def U_ss(params):\n",
    "    \"\"\"A single (L=1) ansatz layer (Rotations + CNOTs).\"\"\"\n",
    "    # 1. Trainable Rotations\n",
    "    for i, wire in enumerate(COMP_WIRES):\n",
    "        qml.RY(params[i], wires=wire)\n",
    "    \n",
    "    # 2. Entangling \"Ring\"\n",
    "    for i in range(len(COMP_WIRES)):\n",
    "        qml.CNOT(wires=[COMP_WIRES[i], COMP_WIRES[(i + 1) % len(COMP_WIRES)]])\n",
    "\n",
    "@qml.qnode(dev, interface='torch', diff_method='parameter-shift')\n",
    "def qmtl_circuit(params, features, context):\n",
    "    # Encode input features into qubits 2, 3, 4\n",
    "    U_in(features)\n",
    "\n",
    "    #Implement shared variational layers\n",
    "    for l in range(N_LAYERS):\n",
    "        U_ss(params['shared'][l])\n",
    "\n",
    "    #TODO: Implement actual control gates for specify layers (allows for superposition of contexts later)\n",
    "    for l in range(N_LAYERS):\n",
    "        U_ss(params['spec'][context][l])\n",
    "\n",
    "    # Measure output qubit (wire 5)\n",
    "    return qml.probs(wires=N_CONTEXT_WIRES + N_INPUT_WIRES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8954f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Features: [ 9.27878004e-05 -6.84386080e-04  1.40243475e-03] Output expectation: tensor([0.9179, 0.0821], dtype=torch.float64)\n",
      "2: ──RY(-0.00)──RY(0.10)─╭●───────╭X──RY(0.50)─╭●───────╭X──RY(-0.10)─╭●───────╭X──RY(-0.20)─╭● ···\n",
      "3: ──RY(0.00)───RY(0.20)─╰X─╭●────│───RY(0.60)─╰X─╭●────│───RY(-0.10)─╰X─╭●────│───RY(-0.20)─╰X ···\n",
      "4: ──RY(0.00)───RY(0.30)────╰X─╭●─│───RY(0.70)────╰X─╭●─│───RY(-0.10)────╰X─╭●─│───RY(-0.20)─── ···\n",
      "5: ──RY(0.40)──────────────────╰X─╰●──RY(0.80)───────╰X─╰●──RY(-0.10)───────╰X─╰●──RY(-0.20)─── ···\n",
      "\n",
      "2: ··· ───────╭X─┤       \n",
      "3: ··· ─╭●────│──┤       \n",
      "4: ··· ─╰X─╭●─│──┤       \n",
      "5: ··· ────╰X─╰●─┤  Probs\n"
     ]
    }
   ],
   "source": [
    "if False: #Test the circuit with dummy parameters\n",
    "\n",
    "    for index, row in df_train[:1].iterrows(): #only first element for testing\n",
    "        print(type(row))\n",
    "        params = dict()\n",
    "        params['shared'] = [[0.1, 0.2, 0.3, 0.4],[0.5, 0.6, 0.7, 0.8]]\n",
    "        params['spec'] = [\n",
    "        [[-0.1, -0.1, -0.1, -0.1], [-0.2, -0.2, -0.2, -0.2]],\n",
    "            [[-0.3, -0.3, -0.3, -0.3], [-0.4, -0.4, -0.4, -0.4]],\n",
    "            [[-0.5, -0.5, -0.5, -0.5], [-0.6, -0.6, -0.6, -0.6]],\n",
    "            [[-0.7, -0.7, -0.7, -0.7], [-0.8, -0.8, -0.8, -0.8]]]\n",
    "        # Execute the circuit\n",
    "        result = qmtl_circuit(params, row['features'], row['context'])\n",
    "        print(\"Features:\", features, \"Output expectation:\", result)\n",
    "\n",
    "        drawing = qml.draw(qmtl_circuit)(params, row['features'], row['context'])\n",
    "        print(drawing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9f4544c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antho\\anaconda3\\envs\\quantum\\Lib\\site-packages\\torch\\nn\\functional.py:3355: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Avg. Loss: 0.0026\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0663,  1.0921, -1.1503,  0.7243],\n",
      "        [-0.7098,  0.2456, -0.2202,  1.4512]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7667, -0.4806, -1.2539,  0.7395],\n",
      "         [ 1.5891,  0.6025, -1.5270,  0.8520]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 2/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0611,  1.1070, -1.1559,  0.7240],\n",
      "        [-0.7098,  0.2484, -0.2511,  1.4537]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7423, -0.4791, -1.2786,  0.7147],\n",
      "         [ 1.5983,  0.5815, -1.5436,  0.8502]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 2/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0611,  1.1070, -1.1559,  0.7240],\n",
      "        [-0.7098,  0.2484, -0.2511,  1.4537]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7423, -0.4791, -1.2786,  0.7147],\n",
      "         [ 1.5983,  0.5815, -1.5436,  0.8502]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 3/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0535,  1.1313, -1.1597,  0.7174],\n",
      "        [-0.7059,  0.2376, -0.2809,  1.4499]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7359, -0.4777, -1.2837,  0.7370],\n",
      "         [ 1.6106,  0.5708, -1.5588,  0.8389]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 3/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0535,  1.1313, -1.1597,  0.7174],\n",
      "        [-0.7059,  0.2376, -0.2809,  1.4499]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7359, -0.4777, -1.2837,  0.7370],\n",
      "         [ 1.6106,  0.5708, -1.5588,  0.8389]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 4/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0562,  1.1412, -1.1635,  0.7142],\n",
      "        [-0.7038,  0.2297, -0.2955,  1.4478]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7268, -0.4739, -1.2844,  0.7312],\n",
      "         [ 1.6156,  0.5599, -1.5693,  0.8296]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 4/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0562,  1.1412, -1.1635,  0.7142],\n",
      "        [-0.7038,  0.2297, -0.2955,  1.4478]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7268, -0.4739, -1.2844,  0.7312],\n",
      "         [ 1.6156,  0.5599, -1.5693,  0.8296]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 5/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0548,  1.1412, -1.1673,  0.7113],\n",
      "        [-0.7021,  0.2285, -0.3046,  1.4466]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7173, -0.4704, -1.2820,  0.7325],\n",
      "         [ 1.6093,  0.5542, -1.5883,  0.8275]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 5/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0548,  1.1412, -1.1673,  0.7113],\n",
      "        [-0.7021,  0.2285, -0.3046,  1.4466]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7173, -0.4704, -1.2820,  0.7325],\n",
      "         [ 1.6093,  0.5542, -1.5883,  0.8275]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 6/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0612,  1.1452, -1.1716,  0.7099],\n",
      "        [-0.7014,  0.2172, -0.3023,  1.4467]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7018, -0.4664, -1.2815,  0.7188],\n",
      "         [ 1.6142,  0.5464, -1.5924,  0.8144]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 6/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0612,  1.1452, -1.1716,  0.7099],\n",
      "        [-0.7014,  0.2172, -0.3023,  1.4467]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7018, -0.4664, -1.2815,  0.7188],\n",
      "         [ 1.6142,  0.5464, -1.5924,  0.8144]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 7/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0594,  1.1371, -1.1759,  0.7074],\n",
      "        [-0.7001,  0.2190, -0.3049,  1.4469]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.6896, -0.4631, -1.2782,  0.7199],\n",
      "         [ 1.6176,  0.5426, -1.6137,  0.8158]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 7/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0594,  1.1371, -1.1759,  0.7074],\n",
      "        [-0.7001,  0.2190, -0.3049,  1.4469]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.6896, -0.4631, -1.2782,  0.7199],\n",
      "         [ 1.6176,  0.5426, -1.6137,  0.8158]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 8/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0646,  1.1408, -1.1804,  0.7053],\n",
      "        [-0.6990,  0.2046, -0.2984,  1.4466]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.6758, -0.4593, -1.2731,  0.7176],\n",
      "         [ 1.6299,  0.5375, -1.6171,  0.8003]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 8/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0646,  1.1408, -1.1804,  0.7053],\n",
      "        [-0.6990,  0.2046, -0.2984,  1.4466]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.6758, -0.4593, -1.2731,  0.7176],\n",
      "         [ 1.6299,  0.5375, -1.6171,  0.8003]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 9/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0623,  1.1306, -1.1847,  0.7030],\n",
      "        [-0.6976,  0.2064, -0.2992,  1.4463]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.6631, -0.4562, -1.2692,  0.7220],\n",
      "         [ 1.6345,  0.5347, -1.6399,  0.8024]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 9/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0623,  1.1306, -1.1847,  0.7030],\n",
      "        [-0.6976,  0.2064, -0.2992,  1.4463]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.6631, -0.4562, -1.2692,  0.7220],\n",
      "         [ 1.6345,  0.5347, -1.6399,  0.8024]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 10/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0676,  1.1326, -1.1892,  0.7017],\n",
      "        [-0.6966,  0.1910, -0.2919,  1.4462]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.6476, -0.4527, -1.2648,  0.7187],\n",
      "         [ 1.6398,  0.5300, -1.6445,  0.7865]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 10/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0676,  1.1326, -1.1892,  0.7017],\n",
      "        [-0.6966,  0.1910, -0.2919,  1.4462]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.6476, -0.4527, -1.2648,  0.7187],\n",
      "         [ 1.6398,  0.5300, -1.6445,  0.7865]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "N_EPOCHS = 10\n",
    "\n",
    "def init_params():\n",
    "    \"\"\"\n",
    "    Initialize the trainable parameters using torch.nn.Parameter\n",
    "    \"\"\"\n",
    "    n_params_per_layer = len(COMP_WIRES) # 3 input + 1 output = 4\n",
    "    \n",
    "    # --- Shared Parameters ---\n",
    "    # We need L=2 layers of 4 params each\n",
    "    shared_params = torch.randn(N_LAYERS, n_params_per_layer, requires_grad=True)\n",
    "    \n",
    "    # --- Specify Parameters ---\n",
    "    # We need K=4 sets of (L=2 layers * 4 params each)\n",
    "    n_stocks = 2**N_CONTEXT_WIRES\n",
    "    spec_params = torch.randn(n_stocks, N_LAYERS, n_params_per_layer, requires_grad=True)\n",
    "    \n",
    "    # We use torch.nn.ParameterDict to keep them organized\n",
    "    return nn.ParameterDict({\n",
    "        \"shared\": nn.Parameter(shared_params),\n",
    "        \"spec\": nn.Parameter(spec_params)\n",
    "    })\n",
    "\n",
    "#Train the circuit\n",
    "params = init_params()\n",
    "\n",
    "optimizer = torch.optim.Adam(params.values(), lr=LEARNING_RATE)\n",
    "\n",
    "#KL-Divergence used\n",
    "loss_fn = nn.KLDivLoss()\n",
    "\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "    \n",
    "    # --- Run Epochs ---\n",
    "for epoch in range(N_EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # We loop through the training set one sample at a time\n",
    "    # (This is Stochastic Gradient Descent, Batch Size = 1)\n",
    "    # TODO: Increase batch size for more efficiency\n",
    "    for index, row in df_train.iloc[:50].iterrows(): #Only first 50 for testing purposes\n",
    "        \n",
    "        # Create the one-hot true label vector [P(0), P(1)]\n",
    "        y_true_onehot = torch.tensor([0.0, 0.0])\n",
    "        y_true_onehot[row['label']] = 1.0\n",
    "        \n",
    "        # 2. Run circuit, y_pred = [P(0), P(1)]\n",
    "        y_pred = qmtl_circuit(params, row['features'], row['context'])\n",
    "        \n",
    "        # Calculate Loss\n",
    "        # **CRITICAL**: KLDivLoss expects log-probabilities\n",
    "        loss = loss_fn(torch.log(y_pred), y_true_onehot)\n",
    "        \n",
    "        # Calculate Gradients\n",
    "        # PyTorch and PennyLane work together to run the parameter-shift rule for ALL parameters.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(df_train)\n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS} - Avg. Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"--- Training Complete ---\")\n",
    "    print(\"Final Parameters (Shared):\")\n",
    "    print(params['shared'])\n",
    "    print(\"Final Parameters (Specify):\")\n",
    "    print(params['spec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f8b9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
