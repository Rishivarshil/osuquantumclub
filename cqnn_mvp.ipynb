{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "453dd7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       AAPL      AMZN      GOOG      MSFT\n",
      "0 -0.000076  0.005513  0.007070  0.002016\n",
      "1  0.002013  0.001940  0.001570  0.003806\n",
      "2  0.004917  0.006963  0.006282  0.005351\n",
      "3 -0.001616  0.006220  0.001852  0.000443\n",
      "4 -0.000050  0.002026 -0.000267 -0.000295\n"
     ]
    }
   ],
   "source": [
    "VERBOSE = True # Testing\n",
    "\n",
    "import pandas as pd\n",
    "# load data from log-returns\n",
    "\n",
    "df_returns = pd.read_csv('squashed_log_returns.csv')\n",
    "\n",
    "print(df_returns.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b86d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   context                                       raw_features  raw_label\n",
      "0        0  [-7.570098556833607e-05, 0.002012513590532, 0....  -0.001616\n",
      "1        0  [0.002012513590532, 0.0049166646207804, -0.001...  -0.000050\n",
      "2        0  [0.0049166646207804, -0.0016160763885611, -4.9...  -0.000100\n",
      "3        0  [-0.0016160763885611, -4.977979782626501e-05, ...   0.002460\n",
      "4        0  [-4.977979782626501e-05, -9.973891197457896e-0...   0.004462\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#split into windows\n",
    "WINDOW_SIZE = 4 #time windows of 4\n",
    "\n",
    "df = pd.DataFrame(columns=['context','raw_features','raw_label'])\n",
    "    \n",
    "for context, ticker in enumerate(df_returns.columns):\n",
    "        \n",
    "    stock_series = df_returns[ticker]\n",
    "        \n",
    "    # Slide a window across this one stock's time series\n",
    "    # We stop (window_size - 1) from the end\n",
    "    for i in range(len(stock_series) - WINDOW_SIZE + 1):\n",
    "            \n",
    "        # The full window (e.g., 4 log-returns)\n",
    "        window = stock_series.iloc[i : i + WINDOW_SIZE]\n",
    "            \n",
    "        # Features are the first N-1 (e.g., 3)\n",
    "        features = window[:-1].values\n",
    "            \n",
    "        # Label is the last one (e.g., the 4th)\n",
    "        label = window.iloc[-1]\n",
    "\n",
    "        df.loc[len(df)] = [context, features, label]\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1a93f434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample rows after binning:\n",
      "\n",
      "Context 0 (edges=[-0.05973468 -0.00531524  0.0491042 ]):\n",
      "  raw_features: [-7.57009856e-05  2.01251359e-03  4.91666462e-03]\n",
      "  binned features: [1 1 1]\n",
      "  raw_label: -0.0016160763885611\n",
      "  binned label: 1\n",
      "\n",
      "Context 0 (edges=[-0.05973468 -0.00531524  0.0491042 ]):\n",
      "  raw_features: [ 0.00201251  0.00491666 -0.00161608]\n",
      "  binned features: [1 1 1]\n",
      "  raw_label: -4.977979782626501e-05\n",
      "  binned label: 1\n",
      "\n",
      "Context 0 (edges=[-0.05973468 -0.00531524  0.0491042 ]):\n",
      "  raw_features: [ 4.91666462e-03 -1.61607639e-03 -4.97797978e-05]\n",
      "  binned features: [1 1 1]\n",
      "  raw_label: -9.973891197457896e-05\n",
      "  binned label: 1\n",
      "\n",
      "Context 0 (edges=[-0.05973468 -0.00531524  0.0491042 ]):\n",
      "  raw_features: [-1.61607639e-03 -4.97797978e-05 -9.97389120e-05]\n",
      "  binned features: [1 1 1]\n",
      "  raw_label: 0.0024599308278872\n",
      "  binned label: 1\n",
      "\n",
      "Context 0 (edges=[-0.05973468 -0.00531524  0.0491042 ]):\n",
      "  raw_features: [-4.97797978e-05 -9.97389120e-05  2.45993083e-03]\n",
      "  binned features: [1 1 1]\n",
      "  raw_label: 0.0044615553560915\n",
      "  binned label: 1\n"
     ]
    }
   ],
   "source": [
    "# Convert features and labels to N_BINS using context-specific min/max ranges\n",
    "import numpy as np\n",
    "\n",
    "# Number of bins (change this to >2 to get more than binary bins)\n",
    "N_BINS = 2\n",
    "\n",
    "# Get per-stock min/max from the original log-returns table\n",
    "return_mins = df_returns.min()\n",
    "return_maxs = df_returns.max()\n",
    "\n",
    "# Precompute bin edges for each context (index corresponds to context integer)\n",
    "edges_per_context = [np.linspace(return_mins.iloc[i], return_maxs.iloc[i], N_BINS + 1)\n",
    "                     for i in range(len(return_mins))]\n",
    "\n",
    "# Helper: bin an ndarray of values for a given context\n",
    "def bin_array(arr, context):\n",
    "    ctx = int(context)\n",
    "    edges = edges_per_context[ctx]\n",
    "    # interior edges (length = N_BINS-1). np.digitize with these yields indices 0..N_BINS-1\n",
    "    interior = edges[1:-1]\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    if interior.size == 0:\n",
    "        # degenerate case (N_BINS <= 2), use midpoint logic\n",
    "        midpoint = edges[1]\n",
    "        return np.array([int(x > midpoint) for x in arr], dtype=int)\n",
    "    return np.digitize(arr, interior).astype(int)\n",
    "\n",
    "# Helper: bin a single scalar value\n",
    "def bin_scalar(val, context):\n",
    "    ctx = int(context)\n",
    "    edges = edges_per_context[ctx]\n",
    "    interior = edges[1:-1]\n",
    "    v = float(val)\n",
    "    if interior.size == 0:\n",
    "        midpoint = edges[1]\n",
    "        return int(v > midpoint)\n",
    "    return int(np.digitize([v], interior)[0])\n",
    "\n",
    "# Apply binning to each row: features (elementwise) and label (scalar)\n",
    "df['features'] = df.apply(lambda row: bin_array(row['raw_features'], row['context']), axis=1)\n",
    "df['label'] = df.apply(lambda row: bin_scalar(row['raw_label'], row['context']), axis=1)\n",
    "\n",
    "if VERBOSE: #Testing\n",
    "    # Verification prints\n",
    "    print('\\nSample rows after binning:')\n",
    "    for _, row in df.head(5).iterrows():\n",
    "        ctx = int(row['context'])\n",
    "        print(f\"\\nContext {ctx} (edges={edges_per_context[ctx]}):\")\n",
    "        print(f\"  raw_features: {row['raw_features']}\")\n",
    "        print(f\"  binned features: {row['features']}\")\n",
    "        print(f\"  raw_label: {row['raw_label']}\")\n",
    "        print(f\"  binned label: {row['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d15acdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4816, 5)\n",
      "(1204, 5)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "\n",
    "TRAIN_RATIO = 0.8  # take the first 80% of df as requested\n",
    "\n",
    "# Compute split index using the row order (first 80%)\n",
    "n_rows = len(df)\n",
    "split_index = int(n_rows * TRAIN_RATIO)\n",
    "\n",
    "# First 80% (preserve original order); create df_test for remainder\n",
    "df_train = df.iloc[:split_index].reset_index(drop=True)\n",
    "df_test = df.iloc[split_index:].reset_index(drop=True)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5c4c5f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probability distribution of feature patterns:\n",
      "--------------------------------------------------\n",
      "Pattern    | Probability | Count\n",
      "--------------------------------------------------\n",
      "[0,0,0]    |      0.013 |    64\n",
      "[0,0,1]    |      0.044 |   211\n",
      "[0,1,0]    |      0.039 |   187\n",
      "[0,1,1]    |      0.120 |   578\n",
      "[1,0,0]    |      0.044 |   211\n",
      "[1,0,1]    |      0.115 |   555\n",
      "[1,1,0]    |      0.120 |   579\n",
      "[1,1,1]    |      0.505 |  2431\n",
      "\n",
      "Total probability: 1.000000\n",
      "Total samples: 4816\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze global probability distribution of feature patterns\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get dimensions\n",
    "n_features = df_train['features'].iloc[0].shape[0]  # Should be 3\n",
    "n_patterns = 2**n_features  # 8 possible patterns for 3 binary features\n",
    "\n",
    "# Generate all possible patterns in natural binary order\n",
    "patterns = np.array(list(product([0, 1], repeat=n_features)))\n",
    "pattern_strs = [''.join(map(str, p)) for p in patterns]\n",
    "\n",
    "# Stack all feature vectors into a 2D array\n",
    "all_features = np.stack(df_train['features'].values)\n",
    "\n",
    "# Count occurrences of each pattern\n",
    "counts = np.zeros(n_patterns)\n",
    "for i, pattern in enumerate(patterns):\n",
    "    matches = np.all(all_features == pattern, axis=1)\n",
    "    counts[i] = np.sum(matches)\n",
    "\n",
    "# Convert to probabilities\n",
    "probs = counts / counts.sum()\n",
    "\n",
    "if VERBOSE:\n",
    "    # Print probabilities in natural binary order\n",
    "    print(\"\\nProbability distribution of feature patterns:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Pattern    | Probability | Count\")\n",
    "    print(\"-\" * 50)\n",
    "    for i in range(n_patterns):\n",
    "        pattern_str = f\"[{','.join(map(str, patterns[i]))}]\"\n",
    "        print(f\"{pattern_str:10s} | {probs[i]:10.3f} | {int(counts[i]):5d}\")\n",
    "\n",
    "# Verify distribution sums to 1\n",
    "print(f\"\\nTotal probability: {probs.sum():.6f}\")\n",
    "print(f\"Total samples: {int(counts.sum())}\")\n",
    "\n",
    "# Plot bar chart (keeping natural binary order)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(pattern_strs, probs)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Feature Pattern')\n",
    "plt.title('Global Probability Distribution of Feature Patterns')\n",
    "\n",
    "# Add probability values on top of bars\n",
    "for i, v in enumerate(probs):\n",
    "    plt.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "07be5b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State preparation circuit (hardware-efficient ansatz)\n",
    "import pennylane as qml\n",
    "\n",
    "L_SP = 2 # number of layers for state preparation ansatz\n",
    "N_WIRES_SP = WINDOW_SIZE - 1  # 3 qubits for 3 features\n",
    "\n",
    "dev_sp = qml.device(\"default.qubit\", wires=N_WIRES_SP)\n",
    "\n",
    "@qml.qnode(dev_sp)\n",
    "def sp_circuit(params):\n",
    "\n",
    "    #Hadamard layer\n",
    "    for i in range(N_WIRES_SP):\n",
    "        qml.Hadamard(wires=i)\n",
    "\n",
    "    #Optimization layers\n",
    "    for l in range(L_SP):\n",
    "        for i in range(N_WIRES_SP):\n",
    "            qml.RY(params[2*l*N_WIRES_SP + 2*i], wires=i)\n",
    "            qml.RZ(params[2*l*N_WIRES_SP + 2*i + 1], wires=i)\n",
    "\n",
    "        for j in range(N_WIRES_SP):\n",
    "            qml.CNOT(wires=[j, (j + 1) % N_WIRES_SP])\n",
    "\n",
    "    return qml.probs(wires=range(N_WIRES_SP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "53fff1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SPSA training for state preparation...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\antho\\anaconda3\\envs\\quantum\\Lib\\site-packages\\pennylane\\optimize\\spsa.py:266\u001b[0m, in \u001b[0;36mSPSAOptimizer.compute_grad\u001b[1;34m(self, objective_fn, args, kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 266\u001b[0m     dev_shots \u001b[38;5;241m=\u001b[39m \u001b[43mobjective_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241m.\u001b[39mshots\n\u001b[0;32m    268\u001b[0m     shots \u001b[38;5;241m=\u001b[39m dev_shots \u001b[38;5;28;01mif\u001b[39;00m dev_shots\u001b[38;5;241m.\u001b[39mhas_partitioned_shots \u001b[38;5;28;01melse\u001b[39;00m Shots(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'device'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 36\u001b[0m\n\u001b[0;32m     31\u001b[0m losses_sp \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS_SP):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Single SPSA update step\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     params, loss_val \u001b[38;5;241m=\u001b[39m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_and_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     losses_sp\u001b[38;5;241m.\u001b[39mappend(loss_val)\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Print progress\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\antho\\anaconda3\\envs\\quantum\\Lib\\site-packages\\pennylane\\optimize\\spsa.py:195\u001b[0m, in \u001b[0;36mSPSAOptimizer.step_and_cost\u001b[1;34m(self, objective_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_and_cost\u001b[39m(\u001b[38;5;28mself\u001b[39m, objective_fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Update the parameter array :math:`\\hat{\\theta}_k` with one step of the\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m    optimizer and return the step and the corresponding objective function. The number\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    of steps stored by the ``k`` attribute of the optimizer is counted internally when calling ``step_and_cost`` and ``cost``.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m        objective function output prior to the step.\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 195\u001b[0m     g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     new_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_grad(g, args)\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\antho\\anaconda3\\envs\\quantum\\Lib\\site-packages\\pennylane\\optimize\\spsa.py:276\u001b[0m, in \u001b[0;36mSPSAOptimizer.compute_grad\u001b[1;34m(self, objective_fn, args, kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    272\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe objective function must be a scalar function for the gradient \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto be computed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m         )\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43myplus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(  \u001b[38;5;66;03m# pylint: disable=raise-missing-from\u001b[39;00m\n\u001b[0;32m    278\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe objective function must be a scalar function for the gradient \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto be computed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         )\n\u001b[0;32m    281\u001b[0m grad \u001b[38;5;241m=\u001b[39m [(yplus \u001b[38;5;241m-\u001b[39m yminus) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m ck \u001b[38;5;241m*\u001b[39m di) \u001b[38;5;28;01mfor\u001b[39;00m di \u001b[38;5;129;01min\u001b[39;00m delta]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "# Train the hardware-efficient ansatz to prepare the target probability distribution using PennyLane's SPSA optimizer\n",
    "from pennylane.optimize import SPSAOptimizer\n",
    "\n",
    "N_EPOCHS_SP = 100\n",
    "LEARNING_RATE_SP = 0.1\n",
    "DELTA_PERTURB = 0.1\n",
    "INIT_PARAM_SCALE = 0.01\n",
    "\n",
    "\n",
    "target_np = np.array(probs, dtype=np.float64)\n",
    "\n",
    "# Initialize parameters as a NumPy array\n",
    "params = np.random.randn(L_SP*N_WIRES_SP*2).astype(np.float64) * INIT_PARAM_SCALE\n",
    "\n",
    "# Create SPSA optimizer\n",
    "opt = SPSAOptimizer(maxiter=N_EPOCHS_SP, a=LEARNING_RATE_SP, c=DELTA_PERTURB)\n",
    "\n",
    "# Objective for SPSA: receives numpy params and returns a Python float loss\n",
    "def objective(params):\n",
    "    # Run circuit\n",
    "    out = sp_circuit(params)\n",
    "    \n",
    "    # Calculate MSE loss\n",
    "    loss = np.mean((out - target_np) ** 2)\n",
    "    \n",
    "    # Return as a standard Python float for the optimizer\n",
    "    return float(loss)\n",
    "\n",
    "# Training loop using SPSA optimizer\n",
    "print(\"Starting SPSA training for state preparation...\")\n",
    "losses_sp = []\n",
    "\n",
    "for epoch in range(N_EPOCHS_SP):\n",
    "    # Single SPSA update step\n",
    "\n",
    "    params, loss_val = opt.step_and_cost(objective, params)\n",
    "    losses_sp.append(loss_val)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % max(1, (N_EPOCHS_SP // 10)) == 0 or epoch == N_EPOCHS_SP - 1:\n",
    "        print(f\"Epoch {epoch+1}/{N_EPOCHS_SP} - MSE Loss: {loss_val:.6f}\")\n",
    "\n",
    "# Get final distribution\n",
    "final_out = sp_circuit(params)\n",
    "\n",
    "# Detach from autograd graph for final printing/plotting\n",
    "final_out_np = final_out.numpy()\n",
    "target_out_np = target_np.numpy()\n",
    "\n",
    "print(\"\\nTarget probs:\\n\", np.round(target_out_np, 6))\n",
    "print(\"Final prepared probs:\\n\", np.round(final_out_np, 6))\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, len(losses_sp) + 1), losses_sp)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('State Preparation Training Loss (SPSA)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Small sanity-check: L1 distance\n",
    "l1 = np.sum(np.abs(final_out_np - target_out_np))\n",
    "print(f\"L1 distance between target and prepared distribution: {l1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eb426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Build the circuit\n",
    "\n",
    "# 0-1: context qubits\n",
    "# 2-4: input qubits\n",
    "# 5: output qubit\n",
    "N_CONTEXT_WIRES = int(np.log2(df_returns.shape[1]))  # Number of qubits needed to represent stocks\n",
    "N_INPUT_WIRES = df_train['features'].loc[0].shape[0]  # Should be 3\n",
    "N_OUTPUT_WIRES = int(np.log2(N_BINS))\n",
    "N_TOTAL_WIRES = N_CONTEXT_WIRES + N_INPUT_WIRES + N_OUTPUT_WIRES\n",
    "\n",
    "CONTEXT_WIRES = list(range(N_CONTEXT_WIRES))\n",
    "INPUT_WIRES = list(range(N_CONTEXT_WIRES, N_CONTEXT_WIRES + N_INPUT_WIRES))\n",
    "OUTPUT_WIRES = list(range(N_CONTEXT_WIRES + N_INPUT_WIRES, N_TOTAL_WIRES))\n",
    "COMP_WIRES = list(range(N_CONTEXT_WIRES, N_CONTEXT_WIRES + N_OUTPUT_WIRES + N_INPUT_WIRES))\n",
    "\n",
    "N_LAYERS = 2\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "N_EPOCHS = 10\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=N_TOTAL_WIRES)\n",
    "\n",
    "#input layer\n",
    "def U_in(features):\n",
    "    \"\"\"The data encoding block (feature map).\"\"\"\n",
    "    # We use arctan to squash features, as discussed\n",
    "    for i, wire in enumerate(INPUT_WIRES):\n",
    "        qml.RY(features[i], wires=wire)\n",
    "\n",
    "# layer for both either the shared or specify ansatz (input is just trainable parameters)\n",
    "def U_ss(params):\n",
    "    \"\"\"A single (L=1) ansatz layer (Rotations + CNOTs).\"\"\"\n",
    "    # 1. Trainable Rotations\n",
    "    for i, wire in enumerate(COMP_WIRES):\n",
    "        qml.RY(params[i], wires=wire)\n",
    "    \n",
    "    # 2. Entangling \"Ring\"\n",
    "    for i in range(len(COMP_WIRES)):\n",
    "        qml.CNOT(wires=[COMP_WIRES[i], COMP_WIRES[(i + 1) % len(COMP_WIRES)]])\n",
    "\n",
    "@qml.qnode(dev, interface='torch', diff_method='parameter-shift')\n",
    "def qmtl_circuit(params, features, context):\n",
    "    # Encode input features into qubits 2, 3, 4\n",
    "    U_in(features)\n",
    "\n",
    "    #Implement shared variational layers\n",
    "    for l in range(N_LAYERS):\n",
    "        U_ss(params['shared'][l])\n",
    "\n",
    "    #TODO: Implement actual control gates for specify layers (allows for superposition of contexts later)\n",
    "    for l in range(N_LAYERS):\n",
    "        U_ss(params['spec'][context][l])\n",
    "\n",
    "    # Measure output qubit (wire 5)\n",
    "    return qml.probs(wires=N_CONTEXT_WIRES + N_INPUT_WIRES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8954f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Features: [ 9.27878004e-05 -6.84386080e-04  1.40243475e-03] Output expectation: tensor([0.9179, 0.0821], dtype=torch.float64)\n",
      "2: RY(-0.00)RY(0.10)XRY(0.50)XRY(-0.10)XRY(-0.20) \n",
      "3: RY(0.00)RY(0.20)XRY(0.60)XRY(-0.10)XRY(-0.20)X \n",
      "4: RY(0.00)RY(0.30)XRY(0.70)XRY(-0.10)XRY(-0.20) \n",
      "5: RY(0.40)XRY(0.80)XRY(-0.10)XRY(-0.20) \n",
      "\n",
      "2:  X       \n",
      "3:         \n",
      "4:  X       \n",
      "5:  X  Probs\n"
     ]
    }
   ],
   "source": [
    "if False: #Test the circuit with dummy parameters\n",
    "\n",
    "    for index, row in df_train[:1].iterrows(): #only first element for testing\n",
    "        print(type(row))\n",
    "        params = dict()\n",
    "        params['shared'] = [[0.1, 0.2, 0.3, 0.4],[0.5, 0.6, 0.7, 0.8]]\n",
    "        params['spec'] = [\n",
    "        [[-0.1, -0.1, -0.1, -0.1], [-0.2, -0.2, -0.2, -0.2]],\n",
    "            [[-0.3, -0.3, -0.3, -0.3], [-0.4, -0.4, -0.4, -0.4]],\n",
    "            [[-0.5, -0.5, -0.5, -0.5], [-0.6, -0.6, -0.6, -0.6]],\n",
    "            [[-0.7, -0.7, -0.7, -0.7], [-0.8, -0.8, -0.8, -0.8]]]\n",
    "        # Execute the circuit\n",
    "        result = qmtl_circuit(params, row['features'], row['context'])\n",
    "        print(\"Features:\", features, \"Output expectation:\", result)\n",
    "\n",
    "        drawing = qml.draw(qmtl_circuit)(params, row['features'], row['context'])\n",
    "        print(drawing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9f4544c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\antho\\anaconda3\\envs\\quantum\\Lib\\site-packages\\torch\\nn\\functional.py:3355: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Avg. Loss: 0.0026\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0663,  1.0921, -1.1503,  0.7243],\n",
      "        [-0.7098,  0.2456, -0.2202,  1.4512]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7667, -0.4806, -1.2539,  0.7395],\n",
      "         [ 1.5891,  0.6025, -1.5270,  0.8520]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 2/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0611,  1.1070, -1.1559,  0.7240],\n",
      "        [-0.7098,  0.2484, -0.2511,  1.4537]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7423, -0.4791, -1.2786,  0.7147],\n",
      "         [ 1.5983,  0.5815, -1.5436,  0.8502]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 2/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0611,  1.1070, -1.1559,  0.7240],\n",
      "        [-0.7098,  0.2484, -0.2511,  1.4537]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7423, -0.4791, -1.2786,  0.7147],\n",
      "         [ 1.5983,  0.5815, -1.5436,  0.8502]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 3/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0535,  1.1313, -1.1597,  0.7174],\n",
      "        [-0.7059,  0.2376, -0.2809,  1.4499]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7359, -0.4777, -1.2837,  0.7370],\n",
      "         [ 1.6106,  0.5708, -1.5588,  0.8389]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 3/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0535,  1.1313, -1.1597,  0.7174],\n",
      "        [-0.7059,  0.2376, -0.2809,  1.4499]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7359, -0.4777, -1.2837,  0.7370],\n",
      "         [ 1.6106,  0.5708, -1.5588,  0.8389]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 4/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0562,  1.1412, -1.1635,  0.7142],\n",
      "        [-0.7038,  0.2297, -0.2955,  1.4478]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7268, -0.4739, -1.2844,  0.7312],\n",
      "         [ 1.6156,  0.5599, -1.5693,  0.8296]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 4/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0562,  1.1412, -1.1635,  0.7142],\n",
      "        [-0.7038,  0.2297, -0.2955,  1.4478]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7268, -0.4739, -1.2844,  0.7312],\n",
      "         [ 1.6156,  0.5599, -1.5693,  0.8296]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 5/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0548,  1.1412, -1.1673,  0.7113],\n",
      "        [-0.7021,  0.2285, -0.3046,  1.4466]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7173, -0.4704, -1.2820,  0.7325],\n",
      "         [ 1.6093,  0.5542, -1.5883,  0.8275]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 5/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0548,  1.1412, -1.1673,  0.7113],\n",
      "        [-0.7021,  0.2285, -0.3046,  1.4466]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7173, -0.4704, -1.2820,  0.7325],\n",
      "         [ 1.6093,  0.5542, -1.5883,  0.8275]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 6/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0612,  1.1452, -1.1716,  0.7099],\n",
      "        [-0.7014,  0.2172, -0.3023,  1.4467]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7018, -0.4664, -1.2815,  0.7188],\n",
      "         [ 1.6142,  0.5464, -1.5924,  0.8144]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 6/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0612,  1.1452, -1.1716,  0.7099],\n",
      "        [-0.7014,  0.2172, -0.3023,  1.4467]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.7018, -0.4664, -1.2815,  0.7188],\n",
      "         [ 1.6142,  0.5464, -1.5924,  0.8144]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 7/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0594,  1.1371, -1.1759,  0.7074],\n",
      "        [-0.7001,  0.2190, -0.3049,  1.4469]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.6896, -0.4631, -1.2782,  0.7199],\n",
      "         [ 1.6176,  0.5426, -1.6137,  0.8158]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 7/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0594,  1.1371, -1.1759,  0.7074],\n",
      "        [-0.7001,  0.2190, -0.3049,  1.4469]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.6896, -0.4631, -1.2782,  0.7199],\n",
      "         [ 1.6176,  0.5426, -1.6137,  0.8158]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 8/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0646,  1.1408, -1.1804,  0.7053],\n",
      "        [-0.6990,  0.2046, -0.2984,  1.4466]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.6758, -0.4593, -1.2731,  0.7176],\n",
      "         [ 1.6299,  0.5375, -1.6171,  0.8003]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 8/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0646,  1.1408, -1.1804,  0.7053],\n",
      "        [-0.6990,  0.2046, -0.2984,  1.4466]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.6758, -0.4593, -1.2731,  0.7176],\n",
      "         [ 1.6299,  0.5375, -1.6171,  0.8003]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 9/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0623,  1.1306, -1.1847,  0.7030],\n",
      "        [-0.6976,  0.2064, -0.2992,  1.4463]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.6631, -0.4562, -1.2692,  0.7220],\n",
      "         [ 1.6345,  0.5347, -1.6399,  0.8024]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 9/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0623,  1.1306, -1.1847,  0.7030],\n",
      "        [-0.6976,  0.2064, -0.2992,  1.4463]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.6631, -0.4562, -1.2692,  0.7220],\n",
      "         [ 1.6345,  0.5347, -1.6399,  0.8024]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 10/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0676,  1.1326, -1.1892,  0.7017],\n",
      "        [-0.6966,  0.1910, -0.2919,  1.4462]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.6476, -0.4527, -1.2648,  0.7187],\n",
      "         [ 1.6398,  0.5300, -1.6445,  0.7865]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n",
      "Epoch 10/10 - Avg. Loss: 0.0025\n",
      "--- Training Complete ---\n",
      "Final Parameters (Shared):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0676,  1.1326, -1.1892,  0.7017],\n",
      "        [-0.6966,  0.1910, -0.2919,  1.4462]], requires_grad=True)\n",
      "Final Parameters (Specify):\n",
      "Parameter containing:\n",
      "tensor([[[-1.6476, -0.4527, -1.2648,  0.7187],\n",
      "         [ 1.6398,  0.5300, -1.6445,  0.7865]],\n",
      "\n",
      "        [[ 0.6428, -0.2284,  1.8288,  1.2051],\n",
      "         [ 1.1416, -1.2565, -0.3173, -3.1662]],\n",
      "\n",
      "        [[ 0.3604, -1.3728,  0.2914, -0.3286],\n",
      "         [-0.8405, -0.2691, -1.3748,  0.3163]],\n",
      "\n",
      "        [[ 1.5240, -1.0766,  1.3405,  1.3979],\n",
      "         [-1.7964,  0.8130,  1.4642, -0.9760]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "N_EPOCHS = 10\n",
    "\n",
    "def init_params():\n",
    "    \"\"\"\n",
    "    Initialize the trainable parameters using torch.nn.Parameter\n",
    "    \"\"\"\n",
    "    n_params_per_layer = len(COMP_WIRES) # 3 input + 1 output = 4\n",
    "    \n",
    "    # --- Shared Parameters ---\n",
    "    # We need L=2 layers of 4 params each\n",
    "    shared_params = torch.randn(N_LAYERS, n_params_per_layer, requires_grad=True)\n",
    "    \n",
    "    # --- Specify Parameters ---\n",
    "    # We need K=4 sets of (L=2 layers * 4 params each)\n",
    "    n_stocks = 2**N_CONTEXT_WIRES\n",
    "    spec_params = torch.randn(n_stocks, N_LAYERS, n_params_per_layer, requires_grad=True)\n",
    "    \n",
    "    # We use torch.nn.ParameterDict to keep them organized\n",
    "    return nn.ParameterDict({\n",
    "        \"shared\": nn.Parameter(shared_params),\n",
    "        \"spec\": nn.Parameter(spec_params)\n",
    "    })\n",
    "\n",
    "#Train the circuit\n",
    "params = init_params()\n",
    "\n",
    "optimizer = torch.optim.Adam(params.values(), lr=LEARNING_RATE)\n",
    "\n",
    "#KL-Divergence used\n",
    "loss_fn = nn.KLDivLoss()\n",
    "\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "    \n",
    "    # --- Run Epochs ---\n",
    "for epoch in range(N_EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # We loop through the training set one sample at a time\n",
    "    # (This is Stochastic Gradient Descent, Batch Size = 1)\n",
    "    # TODO: Increase batch size for more efficiency\n",
    "    for index, row in df_train.iloc[:50].iterrows(): #Only first 50 for testing purposes\n",
    "        \n",
    "        # Create the one-hot true label vector [P(0), P(1)]\n",
    "        y_true_onehot = torch.tensor([0.0, 0.0])\n",
    "        y_true_onehot[row['label']] = 1.0\n",
    "        \n",
    "        # 2. Run circuit, y_pred = [P(0), P(1)]\n",
    "        y_pred = qmtl_circuit(params, row['features'], row['context'])\n",
    "        \n",
    "        # Calculate Loss\n",
    "        # **CRITICAL**: KLDivLoss expects log-probabilities\n",
    "        loss = loss_fn(torch.log(y_pred), y_true_onehot)\n",
    "        \n",
    "        # Calculate Gradients\n",
    "        # PyTorch and PennyLane work together to run the parameter-shift rule for ALL parameters.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(df_train)\n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS} - Avg. Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"--- Training Complete ---\")\n",
    "    print(\"Final Parameters (Shared):\")\n",
    "    print(params['shared'])\n",
    "    print(\"Final Parameters (Specify):\")\n",
    "    print(params['spec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f8b9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
